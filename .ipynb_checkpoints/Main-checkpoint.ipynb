{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import string\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I am pretty new to Machine Learning (my only background is the Online Coursera Course from Andrew Ng), so this is my plan for this project. \n",
    "\n",
    "1. I am going to first analyze the data shown in order to get a better feel of the data itself and what its structure is. I will do this using pandas and matplot lib. Relevant figures will be shown in Feature Analysis Document. Additionally, I will update the data by creating new features that would help in computation.\n",
    "\n",
    "2. After analyzing features, I will create two machine learning models from scratch, a decision tree, trained and tested recursively using the Gini Impurity metric as well as a logistic regression model. These two models will help me predict the titanic result for the test data. \n",
    "\n",
    "3. In each model I will try to avoid overfitting, this will be done through regularization in the case of logistic regression, and by creating a min node cap for the decision tree. \n",
    "\n",
    "4. I will split the training data into two sets, Regular and Cross-Validation, using a 80 - 20 split. After evaluating the data on each set, using the F-score metric, I will tweak the regularization terms accordingly in order to produce the best results. I will also compare my results (for logistic regression) to well known logistic regression library, sklearn. \n",
    "\n",
    "5. Finally, I will do the same to the test data and submit my scores on Kaggle.\n",
    "\n",
    "\n",
    "All of the models will be shown in the code below. However, if you wish to check out how features were analyzed, you may look at the feature analysis document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f-score Metrix\n",
    "\n",
    "def calc_f_score(y_pred, y_test):\n",
    "    true_positive = 0\n",
    "    false_negatives = 0\n",
    "    false_positives = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_test[i] == 1:\n",
    "            true_positive += 1\n",
    "        elif y_pred[i] == 1 and y_test[i] == 0:\n",
    "            false_positives += 1\n",
    "        elif y_pred[i] == 0 and y_test[i] == 1:\n",
    "            false_negatives += 1\n",
    "    precision = true_positive / (true_positive + false_positives)\n",
    "    recall = true_positive/ (true_positive + false_negatives)\n",
    "    return  2 / (1/precision + 1/recall)   \n",
    "\n",
    "#from Feature Analysis'\n",
    "data = pd.read_csv('Titanic/train.csv')\n",
    "\n",
    "def avg(lst):\n",
    "    return sum(lst)/ len(lst)\n",
    "\n",
    "def change_sex_to_number(gender):\n",
    "    if gender == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def change_embarked_to_number(embarked):\n",
    "    if embarked == 'S':\n",
    "        return 1\n",
    "    elif embarked == 'C':\n",
    "        return 2\n",
    "    elif embarked == 'Q':\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def add_feat(data):\n",
    "    \n",
    "    titles = ['Mrs','Mr', 'Miss', 'Ms', 'Master', 'Dona', 'Captain', 'Rev', 'Don', 'Dr', 'Major', 'Mlle', 'Col', 'Jonkheer', 'Mme', 'Capt', 'Countess' , 'Sir', 'Mme'] \n",
    "\n",
    "    simp_title = {'Mr': 1, 'Mrs': 2, 'Master': 3, 'Miss': 4, 'Army': 5, 'Academic': 6, 'Royal': 7}\n",
    "            \n",
    "    data['Sex_Number'] = [change_sex_to_number(sex) for sex in data['Sex']]\n",
    "    data['Embarked_number'] = [change_embarked_to_number(embarked) for embarked in data['Embarked']]\n",
    "    data['Name_Plain'] = [\"\".join(l for l in b if l not in string.punctuation) for b in data['Name']]\n",
    "    data['Very_Young'] = [int(b <= 5) for b in data['Age']]\n",
    "    data['Title'] = [[word for word in sting.split() if word in titles][0] for sting in data['Name_Plain']]\n",
    "    data['Title'] = data['Title'].replace(['Capt', 'Col', 'Major'], 'Army')\n",
    "    data['Title'] = data['Title'].replace(['Dr', 'Rev'], 'Academic')\n",
    "    data['Title'] = data['Title'].replace(['Countess', 'Lady', 'Sir', 'Jonkheer', 'Don', 'Dona'], 'Royal')\n",
    "    data['Title'] = data['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
    "    data['Title_Numeric'] = [simp_title[sting] for sting in data['Title']]\n",
    "\n",
    "\n",
    "\n",
    "def change_age(data):\n",
    "    nul = data['Age'].isnull()\n",
    "    old_male_avg = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 1])\n",
    "    \n",
    "    old_female_avg = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 2])\n",
    "    \n",
    "    young_female_avg = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 4])\n",
    "    young_male_avg = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 3])\n",
    "    army_average = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 5])\n",
    "    royal_average =avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 7])\n",
    "    academic_average = avg([data['Age'][index] for index in range(len(data['Age'])) if not nul[index] and data['Title_Numeric'][index] == 6])\n",
    "    age_avg = { 1: old_male_avg, 2: old_female_avg, 4: young_female_avg, 3: young_male_avg, 5: army_average, 6: academic_average, 7: royal_average}\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    for index in range(len(nul)):\n",
    "        if nul[index]:\n",
    "            if data['Title_Numeric'][index] in age_avg:\n",
    "                data['Age'][index] = age_avg[data['Title_Numeric'][index]]\n",
    "      \n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "#From Decision Tree\n",
    "\n",
    "class DecisionTree:\n",
    "    empty_positive = [1]\n",
    "    empty_negative = [0]\n",
    "    empty = []\n",
    "    threshold = 0.05\n",
    "    def __init__(self, total = 1):\n",
    "        self.right = self.empty\n",
    "        self.left  = self.empty\n",
    "        DecisionTree.threshold = DecisionTree.threshold * total\n",
    "    def __str__(self):\n",
    "        if self.left == self.empty_positive:\n",
    "            left = '(+)'\n",
    "        elif self.left == self.empty_negative:\n",
    "            left = '(-)'\n",
    "        else:\n",
    "            left = str(self.left)\n",
    "        if self.right == self.empty_positive:\n",
    "            right = '(+)'\n",
    "        elif self.right == self.empty_negative:\n",
    "            right = '(-)'\n",
    "        else:\n",
    "            right = str(self.right)\n",
    "        return '(' + str(self.split) + ', ' + str(self.col) + ')\\n' + left + '\\n' + right\n",
    "    \n",
    "   \n",
    "    \n",
    "    def test(self, data_pt):\n",
    "        assert self is not self.empty\n",
    "        if data_pt[self.col] < self.split:\n",
    "            if self.right == self.empty_positive:\n",
    "                return 1\n",
    "            elif self.right == self.empty_negative:\n",
    "                return 0\n",
    "            else:\n",
    "                return self.right.test(data_pt)\n",
    "        else:\n",
    "            if self.left == self.empty_positive:\n",
    "                return 1\n",
    "            elif self.left == self.empty_negative:\n",
    "                return 0\n",
    "            else:\n",
    "                return self.left.test(data_pt)\n",
    "    \n",
    "    def calculate_gini_impurity(self, X, y, split, col):\n",
    "        p_right = 0\n",
    "        num_right = 0\n",
    "        p_left = 0\n",
    "        num_left = 0\n",
    "        #print('Col: ', col)\n",
    "        \n",
    "        for index in range(len(X[:, col])):\n",
    "                      \n",
    "            if X[index, col] > split:\n",
    "                num_right += 1\n",
    "                if y[0, index] != 0:\n",
    "                    p_right += 1\n",
    "            else:\n",
    "                num_left += 1\n",
    "                \n",
    "                if y[0, index] != 0:\n",
    "                    p_left += 1\n",
    "                \n",
    "        \n",
    "        \n",
    "        p_left = p_left / num_left\n",
    "        p_right = p_right / num_right\n",
    "        #print('PLeft: ', p_left)\n",
    "        #print('PRight: ', p_right)\n",
    "        \n",
    "        total_impurity = ((p_left - p_left*p_left) * num_left + (p_right - p_right * p_right) * num_right)/ (num_left + num_right)\n",
    "        #print('Total Impurity: ', total_impurity)\n",
    "        \n",
    "        return total_impurity\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, X_and_y):\n",
    "        self.matrix = X_and_y\n",
    "        X = np.matrix(X_and_y[:, :-1])\n",
    "        \n",
    "        y = np.matrix(X_and_y[:, -1])\n",
    "        \n",
    "        n, m = np.shape(X)\n",
    "        col_number = 0\n",
    "        split_value= 0\n",
    "        min_impur = 1\n",
    "        for col in range(m):\n",
    "            mini = min(X[:, col])\n",
    "            maxi = max(X[:, col])\n",
    "            interval = (maxi - mini)/10\n",
    "            if interval == 0:\n",
    "                continue\n",
    "            for _ in range(10):\n",
    "                \n",
    "                impur = self.calculate_gini_impurity(X, y, mini[0,0], col)\n",
    "                \n",
    "                if impur <= min_impur:\n",
    "                    min_impur = impur\n",
    "                    col_number = col\n",
    "                    split_value = mini[0,0]\n",
    "                mini += interval\n",
    "            \n",
    "        self.split = split_value\n",
    "        self.col = col_number\n",
    "        \n",
    "        right_matrix = X_and_y[ (X_and_y[:, col_number] < self.split).ravel(), :]\n",
    "        left_matrix = X_and_y[ (X_and_y[:, col_number] >= self.split).ravel(), :]\n",
    "        \n",
    "        \n",
    "        if len(right_matrix) > self.threshold and len(left_matrix) > 0: \n",
    "            self.right = DecisionTree()\n",
    "            self.right.train(right_matrix)\n",
    "        else:\n",
    "            if np.mean(right_matrix[:, -1]) < 0.5:\n",
    "                self.right = self.empty_negative\n",
    "            else:\n",
    "                self.right = self.empty_positive\n",
    "        if len(left_matrix) > self.threshold and len(right_matrix) > 0:\n",
    "            self.left = DecisionTree()\n",
    "            self.left.train(left_matrix)\n",
    "        else:\n",
    "            if np.mean(left_matrix[:, -1]) < 0.5:\n",
    "                self.left = self.empty_negative\n",
    "            else:\n",
    "                self.left = self.empty_positive\n",
    "            \n",
    "        \n",
    "#From LogisticRegression\n",
    "\n",
    "\n",
    "def square(x):\n",
    "        return x*x\n",
    "    \n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, regularized = True):\n",
    "        self.regularized = regularized\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-1 * x))\n",
    "    def logistic_function(self, vec, theta):\n",
    "        return self.sigmoid(np.dot(vec,theta))\n",
    "    \n",
    "    def single_value_cost(self, x_vector, theta, y_value):\n",
    "        x_vector = np.append(x_vector, 1)\n",
    "        \n",
    "        return y_value *  np.log(self.logistic_function(x_vector, theta)) + (1 - y_value) *  np.log( 1 - self.logistic_function(x_vector, theta))\n",
    "    def gradient(self,theta, X, y):  \n",
    "        theta = np.matrix(theta)\n",
    "        X = np.matrix(X)\n",
    "        y = np.matrix(y)\n",
    "        n,m = X.shape # for generality\n",
    "        X0 = np.ones((n,1))\n",
    "        X= np.hstack((X,X0))\n",
    "        y = y.T\n",
    "        parameters = int(theta.ravel().shape[1])\n",
    "        grad = np.zeros(parameters)\n",
    "        \n",
    "        error = self.sigmoid(X * theta.T) - y\n",
    "        \n",
    "        for i in range(parameters):\n",
    "            term = np.multiply(error, X[:,i])\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "\n",
    "        return grad \n",
    "    def reg_gradient(self, X, y, theta, lamb):\n",
    "        \n",
    "        return self.gradient(theta, X, y) + lamb/(len(y)) * np.append(np.array(theta[:-1]), [0])\n",
    "\n",
    "    def cost_function(self, x, theta, y):\n",
    "        m = len(y)\n",
    "        return (-1/(m)) * sum([self.single_value_cost(x[i], theta, y[i]) for i in range(m)])\n",
    "    \n",
    "    def reg_cost_function(self, x, theta, y, lamb):\n",
    "        \n",
    "        return self.cost_function(x, theta, y) + lamb/(2 *len(y)) * np.dot(theta[:-1], theta[:-1])\n",
    "    \n",
    "    def grad_check(self, x, y, index, theta, lamb):\n",
    "        \n",
    "        func_before = self.reg_cost_function(x, theta, y, lamb)\n",
    "        theta[index] += 0.0000001\n",
    "        func_after = self.reg_cost_function(x, theta, y, lamb)\n",
    "        theta[index] -= 0.0000001\n",
    "        return (func_after- func_before) / 0.0000001\n",
    "    \n",
    "    def update_theta(self, theta, alpha, x, y, lamb):\n",
    "        \n",
    "        gradient = self.reg_gradient(x, y, theta, lamb)\n",
    "        \n",
    "        #check_gradient = [self.grad_check(x, y, index, theta, lamb) for index in range(len(theta))]\n",
    "        \n",
    "        #print(gradient)\n",
    "        \n",
    "        #print('Check Gradient: ', check_gradient)\n",
    "        #print('Gradient: ', gradient)\n",
    "        \n",
    "        theta = np.array(theta) - alpha * np.array(gradient)\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def initialize_theta(self, x):\n",
    "        return [rd.random() for i in range(len(x[0]) + 1)]\n",
    "    \n",
    "    def scale(self, x):\n",
    "        x = np.array(x)\n",
    "        \n",
    "        x_normed = (x - self.mean)/(self.std)\n",
    "        \n",
    "        return x_normed\n",
    "    \n",
    "    def train(self, x, y, alpha, num_times, lamb):\n",
    "        \n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        self.mean = x.mean(axis = 0)\n",
    "        self.std = x.std(axis = 0)\n",
    "        new_x= self.scale(x)\n",
    "        theta = self.initialize_theta(new_x)\n",
    "        prev = self.reg_cost_function(new_x, theta, y, lamb)\n",
    "        while num_times > 0:\n",
    "            if num_times % 100 == 0:\n",
    "                print('Cost Function Value: ', self.reg_cost_function(new_x, theta, y, lamb))\n",
    "                \n",
    "            theta = self.update_theta(theta, alpha, new_x, y, lamb)\n",
    "            curr = self.reg_cost_function(new_x, theta, y, lamb)\n",
    "            \n",
    "            prev = curr\n",
    "            num_times -= 1\n",
    "            \n",
    "        print(theta)\n",
    "        \n",
    "        \n",
    "        self.theta = theta\n",
    "        #print(self.test(x))\n",
    "    #def regularized_train(self, x, y, alpha, num_times, lamb):\n",
    "    \n",
    "    \n",
    "    def test(self, x):\n",
    "        new_x = self.scale(x)\n",
    "        return np.matrix([np.dot(self.theta, np.matrix(np.append(x_vector, 1)).T) for x_vector in new_x]) \n",
    "                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aagarwal_601/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex        Age  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.000000   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
      "2                               Heikkinen, Miss. Laina  female  26.000000   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
      "4                             Allen, Mr. William Henry    male  35.000000   \n",
      "5                                     Moran, Mr. James    male  32.368090   \n",
      "6                              McCarthy, Mr. Timothy J    male  54.000000   \n",
      "7                       Palsson, Master. Gosta Leonard    male   2.000000   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
      "10                     Sandstrom, Miss. Marguerite Rut  female   4.000000   \n",
      "11                            Bonnell, Miss. Elizabeth  female  58.000000   \n",
      "12                      Saundercock, Mr. William Henry    male  20.000000   \n",
      "13                         Andersson, Mr. Anders Johan    male  39.000000   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.000000   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.000000   \n",
      "16                                Rice, Master. Eugene    male   2.000000   \n",
      "17                        Williams, Mr. Charles Eugene    male  32.368090   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.000000   \n",
      "19                             Masselmani, Mrs. Fatima  female  35.900000   \n",
      "20                                Fynney, Mr. Joseph J    male  35.000000   \n",
      "21                               Beesley, Mr. Lawrence    male  34.000000   \n",
      "22                         McGowan, Miss. Anna \"Annie\"  female  15.000000   \n",
      "23                        Sloper, Mr. William Thompson    male  28.000000   \n",
      "24                       Palsson, Miss. Torborg Danira  female   8.000000   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.000000   \n",
      "26                             Emir, Mr. Farred Chehab    male  32.368090   \n",
      "27                      Fortune, Mr. Charles Alexander    male  19.000000   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female  21.845638   \n",
      "29                                 Todoroff, Mr. Lalio    male  32.368090   \n",
      "..                                                 ...     ...        ...   \n",
      "861                        Giles, Mr. Frederick Edward    male  21.000000   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.000000   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female  21.845638   \n",
      "864                             Gill, Mr. John William    male  24.000000   \n",
      "865                           Bystrom, Mrs. (Karolina)  female  42.000000   \n",
      "866                       Duran y More, Miss. Asuncion  female  27.000000   \n",
      "867               Roebling, Mr. Washington Augustus II    male  31.000000   \n",
      "868                        van Melkebeke, Mr. Philemon    male  32.368090   \n",
      "869                    Johnson, Master. Harold Theodor    male   4.000000   \n",
      "870                                  Balkic, Mr. Cerin    male  26.000000   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.000000   \n",
      "872                           Carlsson, Mr. Frans Olof    male  33.000000   \n",
      "873                        Vander Cruyssen, Mr. Victor    male  47.000000   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.000000   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.000000   \n",
      "876                      Gustafsson, Mr. Alfred Ossian    male  20.000000   \n",
      "877                               Petroff, Mr. Nedelio    male  19.000000   \n",
      "878                                 Laleff, Mr. Kristo    male  32.368090   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.000000   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.000000   \n",
      "881                                 Markun, Mr. Johann    male  33.000000   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika  female  22.000000   \n",
      "883                      Banfield, Mr. Frederick James    male  28.000000   \n",
      "884                             Sutehall, Mr. Henry Jr    male  25.000000   \n",
      "885               Rice, Mrs. William (Margaret Norton)  female  39.000000   \n",
      "886                              Montvila, Rev. Juozas    male  27.000000   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.000000   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  21.845638   \n",
      "889                              Behr, Mr. Karl Howell    male  26.000000   \n",
      "890                                Dooley, Mr. Patrick    male  32.000000   \n",
      "\n",
      "     SibSp  Parch            Ticket      Fare        Cabin Embarked  \\\n",
      "0        1      0         A/5 21171    7.2500          NaN        S   \n",
      "1        1      0          PC 17599   71.2833          C85        C   \n",
      "2        0      0  STON/O2. 3101282    7.9250          NaN        S   \n",
      "3        1      0            113803   53.1000         C123        S   \n",
      "4        0      0            373450    8.0500          NaN        S   \n",
      "5        0      0            330877    8.4583          NaN        Q   \n",
      "6        0      0             17463   51.8625          E46        S   \n",
      "7        3      1            349909   21.0750          NaN        S   \n",
      "8        0      2            347742   11.1333          NaN        S   \n",
      "9        1      0            237736   30.0708          NaN        C   \n",
      "10       1      1           PP 9549   16.7000           G6        S   \n",
      "11       0      0            113783   26.5500         C103        S   \n",
      "12       0      0         A/5. 2151    8.0500          NaN        S   \n",
      "13       1      5            347082   31.2750          NaN        S   \n",
      "14       0      0            350406    7.8542          NaN        S   \n",
      "15       0      0            248706   16.0000          NaN        S   \n",
      "16       4      1            382652   29.1250          NaN        Q   \n",
      "17       0      0            244373   13.0000          NaN        S   \n",
      "18       1      0            345763   18.0000          NaN        S   \n",
      "19       0      0              2649    7.2250          NaN        C   \n",
      "20       0      0            239865   26.0000          NaN        S   \n",
      "21       0      0            248698   13.0000          D56        S   \n",
      "22       0      0            330923    8.0292          NaN        Q   \n",
      "23       0      0            113788   35.5000           A6        S   \n",
      "24       3      1            349909   21.0750          NaN        S   \n",
      "25       1      5            347077   31.3875          NaN        S   \n",
      "26       0      0              2631    7.2250          NaN        C   \n",
      "27       3      2             19950  263.0000  C23 C25 C27        S   \n",
      "28       0      0            330959    7.8792          NaN        Q   \n",
      "29       0      0            349216    7.8958          NaN        S   \n",
      "..     ...    ...               ...       ...          ...      ...   \n",
      "861      1      0             28134   11.5000          NaN        S   \n",
      "862      0      0             17466   25.9292          D17        S   \n",
      "863      8      2          CA. 2343   69.5500          NaN        S   \n",
      "864      0      0            233866   13.0000          NaN        S   \n",
      "865      0      0            236852   13.0000          NaN        S   \n",
      "866      1      0     SC/PARIS 2149   13.8583          NaN        C   \n",
      "867      0      0          PC 17590   50.4958          A24        S   \n",
      "868      0      0            345777    9.5000          NaN        S   \n",
      "869      1      1            347742   11.1333          NaN        S   \n",
      "870      0      0            349248    7.8958          NaN        S   \n",
      "871      1      1             11751   52.5542          D35        S   \n",
      "872      0      0               695    5.0000  B51 B53 B55        S   \n",
      "873      0      0            345765    9.0000          NaN        S   \n",
      "874      1      0         P/PP 3381   24.0000          NaN        C   \n",
      "875      0      0              2667    7.2250          NaN        C   \n",
      "876      0      0              7534    9.8458          NaN        S   \n",
      "877      0      0            349212    7.8958          NaN        S   \n",
      "878      0      0            349217    7.8958          NaN        S   \n",
      "879      0      1             11767   83.1583          C50        C   \n",
      "880      0      1            230433   26.0000          NaN        S   \n",
      "881      0      0            349257    7.8958          NaN        S   \n",
      "882      0      0              7552   10.5167          NaN        S   \n",
      "883      0      0  C.A./SOTON 34068   10.5000          NaN        S   \n",
      "884      0      0   SOTON/OQ 392076    7.0500          NaN        S   \n",
      "885      0      5            382652   29.1250          NaN        Q   \n",
      "886      0      0            211536   13.0000          NaN        S   \n",
      "887      0      0            112053   30.0000          B42        S   \n",
      "888      1      2        W./C. 6607   23.4500          NaN        S   \n",
      "889      0      0            111369   30.0000         C148        C   \n",
      "890      0      0            370376    7.7500          NaN        Q   \n",
      "\n",
      "     Sex_Number  Embarked_number  \\\n",
      "0             0                1   \n",
      "1             1                2   \n",
      "2             1                1   \n",
      "3             1                1   \n",
      "4             0                1   \n",
      "5             0                3   \n",
      "6             0                1   \n",
      "7             0                1   \n",
      "8             1                1   \n",
      "9             1                2   \n",
      "10            1                1   \n",
      "11            1                1   \n",
      "12            0                1   \n",
      "13            0                1   \n",
      "14            1                1   \n",
      "15            1                1   \n",
      "16            0                3   \n",
      "17            0                1   \n",
      "18            1                1   \n",
      "19            1                2   \n",
      "20            0                1   \n",
      "21            0                1   \n",
      "22            1                3   \n",
      "23            0                1   \n",
      "24            1                1   \n",
      "25            1                1   \n",
      "26            0                2   \n",
      "27            0                1   \n",
      "28            1                3   \n",
      "29            0                1   \n",
      "..          ...              ...   \n",
      "861           0                1   \n",
      "862           1                1   \n",
      "863           1                1   \n",
      "864           0                1   \n",
      "865           1                1   \n",
      "866           1                2   \n",
      "867           0                1   \n",
      "868           0                1   \n",
      "869           0                1   \n",
      "870           0                1   \n",
      "871           1                1   \n",
      "872           0                1   \n",
      "873           0                1   \n",
      "874           1                2   \n",
      "875           1                2   \n",
      "876           0                1   \n",
      "877           0                1   \n",
      "878           0                1   \n",
      "879           1                2   \n",
      "880           1                1   \n",
      "881           0                1   \n",
      "882           1                1   \n",
      "883           0                1   \n",
      "884           0                1   \n",
      "885           1                3   \n",
      "886           0                1   \n",
      "887           1                1   \n",
      "888           1                1   \n",
      "889           0                2   \n",
      "890           0                3   \n",
      "\n",
      "                                            Name_Plain  Very_Young     Title  \\\n",
      "0                                Braund Mr Owen Harris           0        Mr   \n",
      "1      Cumings Mrs John Bradley Florence Briggs Thayer           0       Mrs   \n",
      "2                                 Heikkinen Miss Laina           0      Miss   \n",
      "3             Futrelle Mrs Jacques Heath Lily May Peel           0       Mrs   \n",
      "4                               Allen Mr William Henry           0        Mr   \n",
      "5                                       Moran Mr James           0        Mr   \n",
      "6                                McCarthy Mr Timothy J           0        Mr   \n",
      "7                         Palsson Master Gosta Leonard           1    Master   \n",
      "8        Johnson Mrs Oscar W Elisabeth Vilhelmina Berg           0       Mrs   \n",
      "9                      Nasser Mrs Nicholas Adele Achem           0       Mrs   \n",
      "10                       Sandstrom Miss Marguerite Rut           1      Miss   \n",
      "11                              Bonnell Miss Elizabeth           0      Miss   \n",
      "12                        Saundercock Mr William Henry           0        Mr   \n",
      "13                           Andersson Mr Anders Johan           0        Mr   \n",
      "14                  Vestrom Miss Hulda Amanda Adolfina           0      Miss   \n",
      "15                        Hewlett Mrs Mary D Kingcome            0       Mrs   \n",
      "16                                  Rice Master Eugene           1    Master   \n",
      "17                          Williams Mr Charles Eugene           0        Mr   \n",
      "18   Vander Planke Mrs Julius Emelia Maria Vandemoo...           0       Mrs   \n",
      "19                               Masselmani Mrs Fatima           0       Mrs   \n",
      "20                                  Fynney Mr Joseph J           0        Mr   \n",
      "21                                 Beesley Mr Lawrence           0        Mr   \n",
      "22                             McGowan Miss Anna Annie           0      Miss   \n",
      "23                          Sloper Mr William Thompson           0        Mr   \n",
      "24                         Palsson Miss Torborg Danira           0      Miss   \n",
      "25   Asplund Mrs Carl Oscar Selma Augusta Emilia Jo...           0       Mrs   \n",
      "26                               Emir Mr Farred Chehab           0        Mr   \n",
      "27                        Fortune Mr Charles Alexander           0        Mr   \n",
      "28                            ODwyer Miss Ellen Nellie           0      Miss   \n",
      "29                                   Todoroff Mr Lalio           0        Mr   \n",
      "..                                                 ...         ...       ...   \n",
      "861                          Giles Mr Frederick Edward           0        Mr   \n",
      "862    Swift Mrs Frederick Joel Margaret Welles Barron           0       Mrs   \n",
      "863                      Sage Miss Dorothy Edith Dolly           0      Miss   \n",
      "864                               Gill Mr John William           0        Mr   \n",
      "865                               Bystrom Mrs Karolina           0       Mrs   \n",
      "866                         Duran y More Miss Asuncion           0      Miss   \n",
      "867                 Roebling Mr Washington Augustus II           0        Mr   \n",
      "868                          van Melkebeke Mr Philemon           0        Mr   \n",
      "869                      Johnson Master Harold Theodor           1    Master   \n",
      "870                                    Balkic Mr Cerin           0        Mr   \n",
      "871       Beckwith Mrs Richard Leonard Sallie Monypeny           0       Mrs   \n",
      "872                             Carlsson Mr Frans Olof           0        Mr   \n",
      "873                          Vander Cruyssen Mr Victor           0        Mr   \n",
      "874                  Abelson Mrs Samuel Hannah Wizosky           0       Mrs   \n",
      "875                       Najib Miss Adele Kiamie Jane           0      Miss   \n",
      "876                        Gustafsson Mr Alfred Ossian           0        Mr   \n",
      "877                                 Petroff Mr Nedelio           0        Mr   \n",
      "878                                   Laleff Mr Kristo           0        Mr   \n",
      "879          Potter Mrs Thomas Jr Lily Alexenia Wilson           0       Mrs   \n",
      "880           Shelley Mrs William Imanita Parrish Hall           0       Mrs   \n",
      "881                                   Markun Mr Johann           0        Mr   \n",
      "882                         Dahlberg Miss Gerda Ulrika           0      Miss   \n",
      "883                        Banfield Mr Frederick James           0        Mr   \n",
      "884                               Sutehall Mr Henry Jr           0        Mr   \n",
      "885                   Rice Mrs William Margaret Norton           0       Mrs   \n",
      "886                                Montvila Rev Juozas           0  Academic   \n",
      "887                         Graham Miss Margaret Edith           0      Miss   \n",
      "888               Johnston Miss Catherine Helen Carrie           0      Miss   \n",
      "889                                Behr Mr Karl Howell           0        Mr   \n",
      "890                                  Dooley Mr Patrick           0        Mr   \n",
      "\n",
      "     Title_Numeric  \n",
      "0                1  \n",
      "1                2  \n",
      "2                4  \n",
      "3                2  \n",
      "4                1  \n",
      "5                1  \n",
      "6                1  \n",
      "7                3  \n",
      "8                2  \n",
      "9                2  \n",
      "10               4  \n",
      "11               4  \n",
      "12               1  \n",
      "13               1  \n",
      "14               4  \n",
      "15               2  \n",
      "16               3  \n",
      "17               1  \n",
      "18               2  \n",
      "19               2  \n",
      "20               1  \n",
      "21               1  \n",
      "22               4  \n",
      "23               1  \n",
      "24               4  \n",
      "25               2  \n",
      "26               1  \n",
      "27               1  \n",
      "28               4  \n",
      "29               1  \n",
      "..             ...  \n",
      "861              1  \n",
      "862              2  \n",
      "863              4  \n",
      "864              1  \n",
      "865              2  \n",
      "866              4  \n",
      "867              1  \n",
      "868              1  \n",
      "869              3  \n",
      "870              1  \n",
      "871              2  \n",
      "872              1  \n",
      "873              1  \n",
      "874              2  \n",
      "875              4  \n",
      "876              1  \n",
      "877              1  \n",
      "878              1  \n",
      "879              2  \n",
      "880              2  \n",
      "881              1  \n",
      "882              4  \n",
      "883              1  \n",
      "884              1  \n",
      "885              2  \n",
      "886              6  \n",
      "887              4  \n",
      "888              4  \n",
      "889              1  \n",
      "890              1  \n",
      "\n",
      "[891 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "#splitting data into training and cv sets\n",
    "add_feat(data)\n",
    "change_age(data)\n",
    "print(data)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering relevant information from training/testing dataframe\n",
    "def split(data):\n",
    "    things_in_x = ['Pclass', 'Age', 'Fare', 'Parch', 'Sex_Number', 'Embarked_number', 'Title_Numeric', 'Very_Young']\n",
    "    x = np.array([np.array(data[thing]) for thing in things_in_x])\n",
    "    y = np.array(data['Survived'])\n",
    "    return x, y\n",
    "\n",
    "x_train, y = split(train)\n",
    "x = np.matrix(x_train).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Function Value:  0.661375367592\n",
      "Cost Function Value:  0.451914967153\n",
      "Cost Function Value:  0.451913471314\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "Cost Function Value:  0.45191347129\n",
      "[-0.87929817 -0.2443481   0.08078627 -0.15402218  1.20171234  0.19264687\n",
      " -0.03744065  0.35578375 -0.60760625]\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "log = LogisticRegression()\n",
    "log.train(x, y, 1, 1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "x_test, y_test = split(test)\n",
    "x_test = np.matrix(x_test).T\n",
    "k = log.scale(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for vec in k:\n",
    "    l = np.append(vec, 1)\n",
    "    y_pred += [int(round(log.sigmoid(np.dot(log.theta, l))))]\n",
    "\n",
    "     \n",
    "print(calc_f_score(y_pred, y_test))\n",
    "#THIS IS THE ACCURACY OF  THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 943,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Built-in Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn import metrics\n",
    "logreg = lr()\n",
    "logreg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred0 = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
      " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_f_score(y_pred, y_test)\n",
    "#F-score of built in model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like the model I implemented from scratch not only matches the predictions by the sk.learn model, but it acutally seems to improve on them in the cross-validation. This suggests that my algorithmic implementation of logistic regression is correct. Now on to the decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.          22.           7.25       ...,   1.           0.           0.        ]\n",
      " [  1.          38.          71.2833     ...,   2.           0.           1.        ]\n",
      " [  3.          26.           7.925      ...,   4.           0.           1.        ]\n",
      " ..., \n",
      " [  3.          21.84563758  23.45       ...,   4.           0.           0.        ]\n",
      " [  1.          26.          30.         ...,   1.           0.           1.        ]\n",
      " [  3.          32.           7.75       ...,   1.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Splits data into information relevant for tree testing\n",
    "def split_tree(data):\n",
    "    things_in_x = ['Pclass', 'Age', 'Fare', 'Parch', 'Sex_Number', 'Embarked_number', 'Title_Numeric', 'Very_Young', 'Survived']\n",
    "    x_and_y = data[things_in_x]\n",
    "    return x_and_y.as_matrix()\n",
    "x_and_y = split_tree(train)\n",
    "print(x_and_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(x_and_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Training\n",
    "tree = DecisionTree(len(x_and_y))\n",
    "tree.train(x_and_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.6, 6)\n",
      "(2.8, 0)\n",
      "(19.31, 2)\n",
      "(0.6, 3)\n",
      "(25.14625, 2)\n",
      "(-)\n",
      "(-)\n",
      "(+)\n",
      "(6.678, 1)\n",
      "(8.00083, 2)\n",
      "(1.8, 3)\n",
      "(+)\n",
      "(0.9, 4)\n",
      "(-)\n",
      "(+)\n",
      "(+)\n",
      "(+)\n",
      "(4.5, 6)\n",
      "(-)\n",
      "(1.8, 5)\n",
      "(48.2, 1)\n",
      "(+)\n",
      "(0.9, 7)\n",
      "(+)\n",
      "(3.8, 6)\n",
      "(+)\n",
      "(+)\n",
      "(136.75, 2)\n",
      "(+)\n",
      "(35.13, 2)\n",
      "(+)\n",
      "(23.602, 1)\n",
      "(27.4, 1)\n",
      "(+)\n",
      "(+)\n",
      "(+)\n",
      "(2.0, 0)\n",
      "(51.45, 2)\n",
      "(-)\n",
      "(23.8125, 2)\n",
      "(-)\n",
      "(2.0, 5)\n",
      "(15.555, 2)\n",
      "(+)\n",
      "(29.3, 1)\n",
      "(2.9, 0)\n",
      "(8.25, 2)\n",
      "(-)\n",
      "(-)\n",
      "(+)\n",
      "(-)\n",
      "(13.515, 2)\n",
      "(-)\n",
      "(9.45, 2)\n",
      "(29.8, 1)\n",
      "(-)\n",
      "(-)\n",
      "(27.6, 1)\n",
      "(6.93, 2)\n",
      "(8.01582, 2)\n",
      "(-)\n",
      "(7.74916, 2)\n",
      "(-)\n",
      "(-)\n",
      "(-)\n",
      "(23.7, 1)\n",
      "(-)\n",
      "(21.6, 1)\n",
      "(-)\n",
      "(-)\n",
      "(48.5, 1)\n",
      "(-)\n",
      "(32.5, 1)\n",
      "(+)\n",
      "(131.5, 2)\n",
      "(-)\n",
      "(-)\n"
     ]
    }
   ],
   "source": [
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print([tree.test(pt) for pt in split_tree(test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2 = [tree.test(pt) for pt in split_tree(test)]\n",
    "len(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694214876033058"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_f_score(y_pred2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All In All, we see that regardless of which method we use we obtain similar results. Specificially a 70-75% accuracy on this data set. Repeating Multiple Trials, suggests that both methods are fairly equal in their assessment of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
